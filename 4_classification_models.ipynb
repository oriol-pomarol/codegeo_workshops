{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b26640c-6bd6-4fb0-9144-d3e0dc56fdb2",
   "metadata": {},
   "source": [
    "# 4. Classification (ML) Models - learning categories from data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5094522e-2280-4431-8976-fa97fb2ab366",
   "metadata": {},
   "source": [
    "In previous workshops, we focused on regression problems, learning how to predict continuous variables using methods like Random Forest and Neural Networks. Today, we will work on a different type of problem: **classification**. Specifically, we will use machine learning to predict a **sediment categorical characteristic**, based on its **location** and some **physical characteristics**.\n",
    "\n",
    "Our dataset comes from the Geological Survey of the Netherlands and contains descriptions of sediments from the North Sea. Today, we will use a small, pre-processed subset of the dataset, but you can download the full dataset (and many other geological datasets!) at  [DINOloket](https://www.dinoloket.nl/en/subsurface-data). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe0ced0-c747-4883-a1d1-64efaee74030",
   "metadata": {},
   "source": [
    "![DINOloket](images/5_DINOloket.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ad673f-642b-41e5-bf00-5ee861f47897",
   "metadata": {},
   "source": [
    "## 4.1 Problem Definition\n",
    "\n",
    "In this workshop, we will use a dataset containing sample descriptions of sediments from the North Sea. When a sample is collected, the Geological Survey of the Netherlands (GDN as denoted in Dutch) follows a standard method to describe the sediment. Using this \"Standard Drill Description Method\" ([Standaard Boor Beschrijvingsmethode](https://www.grondwatertools.nl/sites/default/files/GDN_SBB-NITG-00-141-A-(3)_20161111.pdf)) the GDN aims to systematically capture multiple characteristics of the collected samples. This method does not only apply to marine sediments, but to any sample that is described by the GDN. Of course, some characteristics only apply to certain types of samples. \n",
    "\n",
    "While some of these descriptions can be made quickly, others require laboratory analysis, which is more time-consuming and resource-intensive. Today, we will try to predict one of the time-consuming measurements (i.e. **Medium sand size category**) based on **location** and some easy-to-describe **sediment properties**.\n",
    "\n",
    "The **Medium sand size category** corresponds to **7** different categories in our dataset based on the size sand size of the sample. This measurement only applies to samples described as sand and those that have a representative portion of sand admixture. \n",
    "\n",
    "| Class            | Sand Median (µm)     | Code  |\n",
    "|-------------------|----------------------|-------|\n",
    "| Extremely fine    | 63 ≤ x < 105           | ZUF   |\n",
    "| Very fine         | 105 ≤ x < 150          | ZZF   |\n",
    "| Moderately fine   | 150  ≤ x < 210          | ZMF   |\n",
    "| Moderately coarse | 210 ≤ x < 300          | ZMG   |\n",
    "| Very coarse       | 300 ≤ x < 420          | ZZG   |\n",
    "| Extremely coarse  | 420  ≤ x< 2000         | ZUG   |\n",
    "\n",
    "**Other categories (ABM = NEN209 and ONB)**:\n",
    "\n",
    "- Coarse category: 210 - < 2000 µm (ZGC)\n",
    "\n",
    "\n",
    "Below are the predictor variables and the target variable for this exercise. Note that the sediment properties (e.g., color, calcareous portion) are also classified according to the categories in the 'Standard Drill Description Method'. If you want more details about these features, refer to the [document](https://www.grondwatertools.nl/sites/default/files/GDN_SBB-NITG-00-141-A-(3)_20161111.pdf) (information in Dutch).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df14f73-2401-44ee-b423-804ff6e691c1",
   "metadata": {},
   "source": [
    "| Feature Name (English)       | Feature Name (Dutch)              | Explanation                                | Reference (Page) |\n",
    "|-------------------------------|------------------------------------|--------------------------------------------|------------------|\n",
    "| Sample ID                    | NITG.nr                           | Sample ID                                 |                  |\n",
    "| X coordinate                 | X.coordinaat                      | X coordinate (CRS:28892)                  |                  |\n",
    "| Y coordinate                 | Y.coordinaat                      | Y coordinate (CRS:28892)                  |                  |\n",
    "| Height with respect to NAP   | Maaiveldhoogte..m.tov.NAP         | Z coordinate (depth)                      |                  |\n",
    "| Color                        | Kleur                             | Color based [SBB format L4]               | 47               |\n",
    "| Calcareous portion           | Kalkgehalte                       | Calcareous content [SBB format L14]       | 75               |\n",
    "| Main soil type               | Hoofdgrondsoort                   | Main soil type based [SBB format L3.1]    | 35               |\n",
    "| Organic portion              | Organische Stof                   | Organic portion [SBB format: L9]          | 65               |\n",
    "| Sand median class            | Zandmediaanklasse                 | Sand median [SBB format: L7.2.1]          | 52               |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809051b7",
   "metadata": {},
   "source": [
    "### Import data and libaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a577096",
   "metadata": {},
   "source": [
    "Scikit-learn already contains most of the classification tools we will need available, so we can keep using the libraries we have already seen so far. We will import the necessary ```sklearn``` tools as we need them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c6c3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2610a128",
   "metadata": {},
   "source": [
    "We have also gathered the data in this repository so you can directly access it. We have selected only a few variables as predictors to simplify the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9307a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data_url =  \"https://raw.githubusercontent.com/oriol-pomarol/codegeo_workshops/main/data/5_classification/sample_descriptions.csv\"\n",
    "data = pd.read_csv(data_url, delimiter=\",\", on_bad_lines='skip')\n",
    "\n",
    "# Define the input and output data\n",
    "input_data = data[['X.coordinate', 'Y.coordinate', 'height_NAP', 'main_soil_type']]\n",
    "output_data = data['sand_median_class']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb41687",
   "metadata": {},
   "source": [
    "Finally, we simply need to clean the data (as it contains NaN values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07993d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the rows with missing values in the predictors\n",
    "drop_rows = input_data.isnull().any(axis=1)\n",
    "input_data = input_data[~drop_rows]\n",
    "output_data = output_data[~drop_rows]\n",
    "print('Number of samples:', len(input_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3371fd28",
   "metadata": {},
   "source": [
    "## 4.2 Predicting probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc2cba3",
   "metadata": {},
   "source": [
    "Bridging the gap between regression and classification task might sound difficult, but it can be achieved with a few simple steps:\n",
    "\n",
    "1. **From class to number**: To be able to use our regression models to predict classes, we need to convert those to numbers. If the classification task is binary (only two possibilities) then this is as simple as using 1 and 0. For multi-class problems a single number is not enough, so we assign each class to a vector with the same size as the number of classes. These vectors are filled with 0s except in one position correspondig to the respective class, which is filled by a 1. This is why this approach is known as *one-hot encoding*.\n",
    "\n",
    "> **Attention**: You might be wondering why not to predict a single number and simply assign the additional classes to another value, for example to 2. This is actually a very bad idea as it would assume that your classes are ordered and would punish errors unevenly during training.\n",
    "\n",
    "2. **From number to probability**: In the previous step we converted our class to a number, but only to 1s or 0s, but our regressor models can only predict continuous numbers. The trick here, is that instead of directly predicting the class, we predict the probability of that particular class. To convert it to our binary outputs we set a *probability threshold*, usually 0.5, for deciding between the two. For multiple classes, we can simply take the one with highest probabilty.\n",
    "\n",
    "3. **From probability to regression**: The final step is how to make our regressor model only predict values (or vectors of values) between one and zero. For binary problems, this can be easily solved by applying the *sigmoid* function to the output of the regression model. For multi-class problems, there is another function called *softmax* that can be applied to our predicted vector to ensure that their components sum up to one, as we would expect from a set of probabilities.\n",
    "\n",
    "Let's experiece this concept in practice by training the simplest classifier available, the logistic regression. In this case, the regressor used for predicting the probabilities is a simple multi-linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c353d2d",
   "metadata": {},
   "source": [
    "### Variable encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2446634",
   "metadata": {},
   "source": [
    "To get started, let's transform our output to a binary variable according to the coarse and fine categories. We need to assign a numerical value (1 or 0) to each of them as we have explained. Let's check how it looks at the moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a9c87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the unique output values\n",
    "print('Initial output unique values:', output_data.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcf752e",
   "metadata": {},
   "source": [
    " As you can see, the word coarse is included in all the names for the coarse soil classes (duh). So, we can easily convert the output to boolean (that is, True or False) by checking if the output string contains ```'coarse'``` or not. Lastly we can check again what unique values there are in the output variable to ensure that we have performed our encoding correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc49a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the output to a binary number\n",
    "output_binary = output_data.str.contains('coarse').astype(int)\n",
    "\n",
    "# Print the unique output values after encoding them\n",
    "print('Output unique values after encoding:', output_binary.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3991437",
   "metadata": {},
   "source": [
    "Ah, we are not done yet unfortunately. One of the predictor variables, ```main_soil_type```, is also categorical! If we try to train with it as is we would get the following error: ```ValueError: could not convert string to float```. To prevent that, we will have to also convert it to a number. Let's see what are the unique values for this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d25f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the unique values from the main_soil_type predictor\n",
    "print('Initial main_soil_type unique values:', input_data['main_soil_type'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e588ceaf",
   "metadata": {},
   "source": [
    "As your can see there are four distinct categories. In this case, we don't want to convert them to binary, as we want to keep the diversity in the inputs, so we will need to use another method: **one-hot encoding**. As explained above, this means transforming the output to a vector containing all 0s except a single 1.\n",
    "\n",
    "Because we want to use it as a predictor, not as a target, this will change a bit how we need to tackle this, as vectors are not valid inputs either. Therefore, we will need to create a separate column for each element of that vector. Another crucial difference is that we need one less column than categories in the predictor variable to avoid multicollinearity (highly correlated input variables).\n",
    "\n",
    "In practice, we already have a function in ```pandas``` which will apply one-hot encoding to the variables of our choice. Let's apply it to ```main_soil_type``` and see how the outcome looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7036589e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pandas get_dummies to encode the main_soil_type predictor\n",
    "input_encoded = pd.get_dummies(input_data, columns=['main_soil_type'], drop_first=True)\n",
    "input_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2f5fe4",
   "metadata": {},
   "source": [
    "### Classifier training and prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b4cc1d",
   "metadata": {},
   "source": [
    "Perfect, now that we have our inputs and outputs as numbers, we can train the logistic regression model on our binary data. As you know, first we will need to split between train and test data to have an independent validation set. Before training, we are going to **normalize** the data. This step is necessary to be able compare the coefficients of the logistic regression and to facilitate training, and can be easily done using the ```StandardScaler()``` class from Scikit-learn. Once the training is complete, we can check the linear coefficients of such model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7309a051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_binary, X_test_binary, y_train_binary, y_test_binary \\\n",
    "    = train_test_split(input_encoded, output_binary, test_size=0.2,\n",
    "                       shuffle=True, random_state=10)\n",
    "\n",
    "# Normalize the data using the standard scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_binary_scaled = scaler.fit_transform(X_train_binary)\n",
    "\n",
    "# Define the logistic regression model and train it with our data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_reg_binary = LogisticRegression()\n",
    "log_reg_binary.fit(X_train_binary_scaled, y_train_binary)\n",
    "\n",
    "# Print the coefficients of the linear regression model\n",
    "coefficients = log_reg_binary.coef_[0]\n",
    "feature_names = X_train_binary.columns\n",
    "pd.DataFrame({'Coefficient': coefficients}, index=feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97b8d4b",
   "metadata": {},
   "source": [
    "This already gives us very important information as to which variables are positively correlated with course sand grains (our output) and which negatively, according to the sign of their coefficients. Because we normalized the data, we also get an idea of which predictors are more impactful in the decision. For example, the heigth seems to be a very good indicator of coarse sand grains.\n",
    "\n",
    "Now let's try to calculate what the prediction should be for the first point in the test set by following the steps that we introduced before - but now the other way around!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6899a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the test data using the same scaler\n",
    "X_test_binary_scaled = scaler.transform(X_test_binary)\n",
    "\n",
    "# Take the first sample from the test data\n",
    "X = X_test_binary_scaled[0:1]\n",
    "\n",
    "# Calculate the output of the regression model using the coefficients\n",
    "regression_output = (np.dot(X, coefficients) + log_reg_binary.intercept_)[0]\n",
    "\n",
    "# Calculate the probability by using the sigmoid function\n",
    "probability = 1 / (1 + np.exp(-regression_output))\n",
    "\n",
    "# Check if the probability exceeds the 0.5 treshold\n",
    "probability_threshold = 0.5\n",
    "predicted_class = int(probability > probability_threshold)\n",
    "\n",
    "print(f'Regression: {regression_output:.4f} -> Probability: {probability:.4f}' \n",
    "      f' -> Predicted class: {predicted_class}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154d0dcc",
   "metadata": {},
   "source": [
    "We can also access the probabilities when we make predictions using the regression model by using the ```.predict_proba()``` method. If we use the usual ```.predict()``` method, a probability threshold of 0.5 is used by default. Let's check that those values correspond to the ones that we have just calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cc4a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the logistic regression returns the same prediction\n",
    "log_reg_proba = log_reg_binary.predict_proba(X)\n",
    "log_reg_class = log_reg_binary.predict(X)\n",
    "\n",
    "# Print the results as a dataframe with both calculated and logistic regression results\n",
    "pd.DataFrame({'Class': [predicted_class, log_reg_class[0]], \n",
    "              'Probability': [probability, log_reg_proba[0, 1]]}, \n",
    "             index=['Calculated', 'Logistic regression'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7846285b",
   "metadata": {},
   "source": [
    "Great! Now that we understood how the prediction was made, let's make a prediction for all the available data to see how well we are doing. To make it more interesting we will check the predictions for a probability threshold of 0.25, 0.5 and 0.75. We can enforce them by predicting the probabilities, then comparing them to the thresholds. How do you think this will influence the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7688e427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a map of the data classes for the original data\n",
    "plt.figure(figsize=(5, 5))\n",
    "scatter = plt.scatter(input_encoded['X.coordinate'],\n",
    "                      input_encoded['Y.coordinate'],\n",
    "                      c=output_binary, cmap='coolwarm')\n",
    "\n",
    "# Add labels and title to the plot\n",
    "plt.xlabel('X coordinate')\n",
    "plt.ylabel('Y coordinate')\n",
    "plt.title('Original data classes')\n",
    "\n",
    "# Add a legend and show the plot\n",
    "legend = scatter.legend_elements()[0]\n",
    "plt.legend(legend, ['Fine', 'Coarse'], loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5196bc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the original data\n",
    "input_encoded_scaled = scaler.transform(input_encoded)\n",
    "\n",
    "# Make predictions using a probability threshold of 0.25, 0.5 and 0.75\n",
    "probability_thresholds = [0.25, 0.5, 0.75]\n",
    "predicted_classes = []\n",
    "for threshold in probability_thresholds:\n",
    "    predicted_probability = log_reg_binary.predict_proba(input_encoded_scaled)[:, 1]\n",
    "    predicted_classes.append((predicted_probability > threshold).astype(int))\n",
    "\n",
    "# Plot a map of the predicted classes with different probability thresholds\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5), sharex=True, sharey=True)\n",
    "\n",
    "for i, (ax, threshold) in enumerate(zip(axes, probability_thresholds)):\n",
    "    scatter = ax.scatter(input_encoded['X.coordinate'], input_encoded['Y.coordinate'], \n",
    "                         c=predicted_classes[i], cmap='coolwarm')\n",
    "    ax.set_xlabel('X coordinate')\n",
    "    ax.set_ylabel('Y coordinate')\n",
    "    ax.set_title(f'Predicted classes with threshold {threshold}')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6692ab6e",
   "metadata": {},
   "source": [
    "As you might have expected, when we increase the probability threshold we predict the class we assigned to 0 (fine grained sands) more often. Essentially, with higher probability threshold we require the model to be increasingly confident that the data is coarse grained sand to classify it as such. We might want to do that, for example, if it would be important that we don't missclassify any fine grained sands. More on that in the following sections. In general, it is best to tune the probability threshold to ensure that we don't overpredict one of the variables, especially if there are many more instances of one class in the data. Keep tuned for a future workshop on how to deal with imbalanced data if you are interested in the topic!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0d94c9",
   "metadata": {},
   "source": [
    "## 4.3 Evaluating the models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71d7db8",
   "metadata": {},
   "source": [
    "### The confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90486ff3",
   "metadata": {},
   "source": [
    "The best way to visualize the results of a classifier model is through a \"confusion matrix\". This is nothing more than a table which columns indicate the classes predicted by the ML model and which rows are the actual classes from the data. This is what it looks like for a binary problem:\n",
    "\n",
    "| Class           | Predicted Positive         | Predicted Negative         |\n",
    "|------------------------|----------------------------|----------------------------|\n",
    "| **Actual Positive**        | **TP** (True Positive)     | **FN** (False Negative)    |\n",
    "| **Actual Negative**        | **FP** (False Positive)    | **TN** (True Negative)     |\n",
    "\n",
    "Although the two binary classes are usually referred to as \"positive\" and \"negative\", it can be any two type of classes, so negative does not necessarily imply *bad*. In our case, for example, the classes refer to either coarse or fine grained sands, so the \"positive\" class is simply the one we assign to 1. We could think of it as if we are predicing whether there are coarse grained sands (positive) or not (negative). Let's generate a confusion matrix with the predictions from our logistic model we trained before **on the test data**. This can be easily computed with the ```confusion_matrix``` function in ```sklearn.metrics```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4fa9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with the logistic regression model for the test set\n",
    "y_pred_binary = log_reg_binary.predict(X_test_binary_scaled)\n",
    "\n",
    "# Create the confusion matrix with the logistic regression model predictions\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "ConfusionMatrixDisplay.from_predictions(y_test_binary, y_pred_binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3315bee",
   "metadata": {},
   "source": [
    "While simply displaying the confusion matrix is already quite informative of model performance, sometimes we want to test on specific metrics. Many of the common metrics used to evaluate the performance of classifier models can actually be computed from the confusion matrix directly. Here are some examples:\n",
    "\n",
    "* **Accuracy**: Probably the best known classification metric, it evaluates the percentage of samples that were classified into the correct class.\n",
    "\n",
    "$$Accuracy = \\frac{TP+TN}{TP+FP+TN+FN}$$\n",
    "\n",
    "* **Precision**: A high precision indicates there were not many false alarms of a specific (in binary, the positive) class.\n",
    "\n",
    "$$Precision = \\frac{TP}{TP+FP}$$\n",
    "\n",
    "* **Recall**: A high recall indicates that there were not many missed cases of a specific (in binary, the positive) class. \n",
    "\n",
    "$$Recall = \\frac{TP}{TP+FN}$$\n",
    "\n",
    "First, try to compute those metrics with pen and paper from the confusion matrix that we just saw. When you are done, you can compare your results with those obtained by using the corresponding Scikit-learn functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd792cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "# Compute the accuracy of the logistic regression model\n",
    "accuracy = accuracy_score(y_test_binary, y_pred_binary)\n",
    "print(f\"The accuracy of the logistic regression model is: {accuracy}\")\n",
    "\n",
    "# Compute the precision of the logistic regression model\n",
    "precision = precision_score(y_test_binary, y_pred_binary)\n",
    "print(f\"The precision of the logistic regression model is: {precision}\")\n",
    "\n",
    "# Compute the recall of the logistic regression model\n",
    "recall = recall_score(y_test_binary, y_pred_binary)\n",
    "print(f\"The recall of the logistic regression model is: {recall}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce754b8",
   "metadata": {},
   "source": [
    "### A more balanced metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58aa20b3",
   "metadata": {},
   "source": [
    "While accuracy is the most straightforward way to determine the performance of a classification model, it might not always be the most suitable. This is especially true when the data that is being predicted is imbalanced, that is, when we have many more instances of one class than the rest. Then, it is common that the classifier learns to predict in favour of the majority class, performing really poorly in the rest. For those cases, there is a better metric that we can use that combines both precision and recall to obtain a more balanced metric for performance, the **F1-score**.\n",
    "\n",
    "$$F1\\ score = 2 \\cdot \\frac{Precision \\cdot Recall}{Precision + Recall}$$\n",
    "\n",
    "Let's compare accuracy and F1-score for the different probability thresholds that we tested in the previous chapter. Which of the two do you feel better relates to model performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22624aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Calculate the F1 score for the results with different probability thresholds\n",
    "for i, threshold in enumerate(probability_thresholds):\n",
    "\n",
    "    # Get the test indexes of the test data from the input_encoded dataframe\n",
    "    test_indixes = input_encoded.index.get_indexer(y_test_binary.index)\n",
    "    predicted_test_binary = predicted_classes[i][test_indixes]\n",
    "\n",
    "    # Calculate the F1 score and accuracy for the given threshold\n",
    "    f1 = f1_score(y_test_binary, predicted_test_binary)\n",
    "    accuracy = accuracy_score(y_test_binary, predicted_test_binary)\n",
    "    print(f\"Threshold: {threshold}\\nAccuracy = {accuracy:.2f}; F1 score = {f1:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b90d385",
   "metadata": {},
   "source": [
    "## 4.4 Multi-class tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e302950",
   "metadata": {},
   "source": [
    "### Predicting multiple classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9871af20",
   "metadata": {},
   "source": [
    "While binary problems are relatively common, many times we would like to predict multiple classes. This is the case, for example, if we want to determine the specific sand grain size category from our data. To make things interesting, we are also going to use more powerful ML algorithms; we will compare the results of:\n",
    "\n",
    "* K-Nearest Neighbors (KNN): This is one of the most commonly known examples of **lazy learners**, which spend little time training but take longer for inference as they have to access the training data. This particular algorithm checks the K nearest neighbours (5 by default) and makes a prediction based on those.\n",
    "\n",
    "* Random Forest (RF): We have already discussed this very common ML algorithm for regressin tasks. Due to the decision-based nature of this model, it works very nicely out of the box with categorical data, making it especially suitable for classification tasks.\n",
    "\n",
    "Again we will need to generate a train and test data with the new outputs and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878f7582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the ANN and RF models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train_multi, X_test_multi, y_train_multi, y_test_multi = \\\n",
    "    train_test_split(input_encoded, output_data, test_size=0.2,\n",
    "                     shuffle=True, random_state=10)\n",
    "\n",
    "# Train the random forest model\n",
    "rf = RandomForestClassifier(random_state=10)\n",
    "rf.fit(X_train_multi, y_train_multi)\n",
    "\n",
    "# Train the k-neighbors classifier model\n",
    "kn = KNeighborsClassifier()\n",
    "kn.fit(X_train_multi, y_train_multi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609cac03",
   "metadata": {},
   "source": [
    "Now that the models are trained, we can show the predictions on a map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519a2cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions for the whole dataset\n",
    "y_pred_rf = rf.predict(input_encoded)\n",
    "y_pred_kn = kn.predict(input_encoded)\n",
    "\n",
    "# Create subplots with shared x and y axes\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5), sharex=True, sharey=True)\n",
    "\n",
    "# Plot the original data classes\n",
    "scatter = axes[0].scatter(input_encoded['X.coordinate'], input_encoded['Y.coordinate'],\n",
    "                          c=pd.Categorical(output_data).codes, cmap='tab20')\n",
    "axes[0].set_xlabel('X coordinate')\n",
    "axes[0].set_ylabel('Y coordinate')\n",
    "axes[0].set_title('Original data classes')\n",
    "\n",
    "# Plot the predicted classes for RF and KNN\n",
    "for i, (ax, y_pred) in enumerate(zip(axes[1:], [y_pred_rf, y_pred_kn])):\n",
    "    ax.scatter(input_encoded['X.coordinate'], input_encoded['Y.coordinate'],\n",
    "                         c=pd.Categorical(output_data).codes, cmap='tab20')\n",
    "    ax.set_xlabel('X coordinate')\n",
    "    ax.set_ylabel('Y coordinate')\n",
    "    ax.set_title(f'Predicted classes with {\"RF\" if i == 0 else \"KNN\"}')\n",
    "\n",
    "# Add a legend and show the plot\n",
    "legend = scatter.legend_elements()[0]\n",
    "output_classes = pd.Categorical(output_data).categories\n",
    "fig.legend(legend, output_classes, loc='center left', bbox_to_anchor=(0.9, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9a8709",
   "metadata": {},
   "source": [
    "### Generalizing the metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fe73ad",
   "metadata": {},
   "source": [
    "The map looks rather similar at first glance. To further evaluate the models, we can use the same tools that we did for the binary problem with some slight differences, for example the confusion matrix. Let's see how it looks for multiple classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205388f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the classes for the test set\n",
    "y_pred_rf_test = rf.predict(X_test_multi)\n",
    "y_pred_kn_test = kn.predict(X_test_multi)\n",
    "\n",
    "# Plot the confusion matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5), sharex=True, sharey=True)\n",
    "\n",
    "# Plot the confusion matrix for the Random Forest\n",
    "ConfusionMatrixDisplay.from_predictions(y_test_multi, y_pred_rf_test, ax=axes[0])\n",
    "axes[0].set_title('Random Forest')\n",
    "axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# Plot the confusion matrix for the K-Neighbors Classifier\n",
    "ConfusionMatrixDisplay.from_predictions(y_test_multi, y_pred_kn_test, ax=axes[1])\n",
    "axes[1].set_title('K-Neighbors Classifier')\n",
    "axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f1de68",
   "metadata": {},
   "source": [
    "Same as with the binary case, the correctly classified data points are stored in the diagonal. However, as we keep adding classes the confusion matrix gets more and more cluttered, increasing the usefulness of using metrics instead. All the metrix that we have seen previously can be generalized to multi-class problems, with some slight differences. For the accuracy, for example, we will simply need to sum all elements in the diagonal and divide by the total data sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98de14e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the accuracy for each of the models\n",
    "accuracy_rf = accuracy_score(y_test_multi, y_pred_rf_test)\n",
    "accuracy_kn = accuracy_score(y_test_multi, y_pred_kn_test)\n",
    "\n",
    "print(f\"Random Forest accuracy: {accuracy_rf:.2f}\")\n",
    "print(f\"K-Neighbors Classifier accuracy: {accuracy_kn:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e0ae66",
   "metadata": {},
   "source": [
    "Recall and precision (and therefore F1-score) are now defined for each class, and need to be averaged for getting a general metric for a model output. However, sometimes we might want to know those metrics only for specific classes. In our example, we might be especially concerned about correctly classifying extremely fine-grained sands, since those can easily infiltrate in machinery that might be installed on these sites reducing their useful lifespan greatly. Which of the two variables should we then compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221f9cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the recall of both models for the extremely fine grain size\n",
    "recall_rf = recall_score(y_test_multi, y_pred_rf_test, labels=['extremely fine'], average=None)\n",
    "recall_kn = recall_score(y_test_multi, y_pred_kn_test, labels=['extremely fine'], average=None)\n",
    "\n",
    "print(f\"Random Forest recall for extremely fine grain size: {recall_rf:.2f}\")\n",
    "print(f\"K-Neighbors Classifier recall for extremely fine grain size: {recall_kn:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bb73ab",
   "metadata": {},
   "source": [
    "## 4.5 Final remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd4b874",
   "metadata": {},
   "source": [
    "In this workshop you have learnt the basics of how to tackle a classification task, from the output definition to training and of course evaluating your model. To cement this knowledge, try and use the same data but choose another variable as your target, for example the main soil type. You can re-use some of the code above. If you feel like having a bit more of a challenge, include also the color column as an additional predictor. Be careful as it is also categorical (and with many categories indeed), so you will need to encode it first. Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99e5aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the predictor and target variables\n",
    "\n",
    "# Split between train and test sets\n",
    "\n",
    "# Train the classification model of your choice\n",
    "\n",
    "# Evaluate the model performance on the test set\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
