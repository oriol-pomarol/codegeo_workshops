{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/oriol-pomarol/codegeo_workshops/blob/main/2_feature_importance/2_feature_importance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# 2 Feature importance - deciphering MLâ€™s predictions\n",
    "During this workshop we will explore three different ways to estimate the importance of our input features on the model outputs: impurity feature importance, permutation feature importance and shap feature importance.\n",
    "\n",
    "## 2.1 Setting up our (random forest) model\n",
    "To begin estimating feature importance we need a model. We will use the same model as in [the previous \"understanding random forest\" workshop](../1_understanding_random_forest/1_understanding_random_forest.ipynb). Surprise, surprise, it's a random forest model. The code blocks below (1) loads the data, (2) splits our data into training and testing datasets, (3) trains our random forest model and (4) provides a simple evaluation of the model performance.\n",
    "\n",
    "Note that this code is near identical to the \"understanding random forest\" workshop, If you have any problems understanding what is happening, please take a look there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "data_url =  \"https://raw.githubusercontent.com/Jignesh1594/CodeGeoworkshop_02_understanding_RF/master/data.csv\"\n",
    "data = pd.read_csv(data_url, delimiter=\",\", on_bad_lines='skip')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.model_selection as model_selection\n",
    "\n",
    "# Split the data into training and test sets\n",
    "input_data = data[['WLHv', 'RH', 'EV24', 'QMeuse', 'QRhine']]\n",
    "output_data = data['value']\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(input_data,\n",
    "                                                                    output_data, \n",
    "                                                                    test_size=0.1,\n",
    "                                                                    shuffle=False)\n",
    "\n",
    "print(f\"Train sample size is {X_train.index.size} and test sample size is {X_test.index.size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.ensemble as ensemble\n",
    "\n",
    "# Train the model\n",
    "model = ensemble.RandomForestRegressor(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.style import use\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "mse = metrics.mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "\n",
    "prediction = {\"date\": y_test.index,\n",
    "              \"actual\": y_test,\n",
    "              \"predicted\": y_pred}\n",
    "prediction = pd.DataFrame(prediction)\n",
    "\n",
    "# Plot the prediction\n",
    "use('ggplot')\n",
    "fig, ax = plt.subplots()\n",
    "prediction.plot.line(x = \"date\",\n",
    "                     ax=ax)\n",
    "ax.set_title(\"Actual vs Predicted\")\n",
    "ax.set_ylabel(\"Water level (m)\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Input features\n",
    "It seems the model has a decent performance, but how is it doing those predictions? To that end we will investigate the importance of the input features for these prediction. Naturally, what is particularly important for this workshop is the input data for the random forest model. Here we use three inputs, called 'WLHv', 'RH', 'EV24', 'QMeuse', 'QRhine', that represent the water level, rainfall, evaporation, discharge in the Meuse and discharge in the Rhine, respectively. The code below will make a quick plot of all the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.style import use\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "for input_feature in X_test.columns:\n",
    "    permutation = {\"date\": X_test.index,\n",
    "                   \"input\": X_test[input_feature]}\n",
    "    permutation = pd.DataFrame(permutation)\n",
    "    \n",
    "    # Plot the permutation\n",
    "    use('ggplot')\n",
    "    fig, ax = plt.subplots()\n",
    "    permutation.plot.line(x=\"date\",\n",
    "                         ax=ax)\n",
    "    ax.set_title(\"Random forest input\")\n",
    "    ax.set_ylabel(f\"{input_feature}\")\n",
    "    ax.set_xlabel(\"Date\")\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Impurity feature importance\n",
    "Impurity feature importance is a special type of feature importance that is only relevant for random forest models and is sometimes called \"gini importance\" or \"mean decrease impurity\". This is the same as used in the [previous workshop on \"understanding random forests\"](../1_understanding_random_forest/1_understanding_random_forest.ipynb). Impurity feature importance is defined as the total decrease in node impurity, weighted by the probability of reaching that node (which is approximated by the proportion of samples reaching that node), averaged over all trees of the ensemble. Simply said, how much does the data in our node remain varied after a split decision (based on a specific input feature) is made.\n",
    "\n",
    "### 2.2.1 Engage some braincells\n",
    "Impurity feature importance is directly calculated by the sklearn package and stored in a property of the *RandomForestRegressor* class. Go to the [RandomForestRegressor documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html), identify the property that stores the feature importance, and add the property in the below code where it shows: \"feature_importance_property\" (first line). If today is not the day to engage your braincells, you can see the answer in the text below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = model.\"feature_importance_property\"\n",
    "\n",
    "# Register the feature importance\n",
    "feature_importance = {\"feature\": X_test.columns.to_list(),\n",
    "                      \"importance\": importances.tolist()}\n",
    "feature_importance = pd.DataFrame(feature_importance)\n",
    "feature_importance.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click to see the solution</summary>\n",
    "    The feature importance property of the *RandomForestRegressor* class is *feature_importances_*\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.style import use\n",
    "\n",
    "# Plot the feature importance\n",
    "use('ggplot')\n",
    "fig, ax = plt.subplots()\n",
    "feature_importance.plot.bar(x = \"feature\",\n",
    "                            y = \"importance\",\n",
    "                            ax=ax)\n",
    "ax.set_title(\"Impurity feature importance\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Permutation feature importance\n",
    "Permutation feature importance is a more generalizable method that can be applied to any type of machine learning model (e.g. random forest, neural network, LSTM) to determine the feature importance. Permutation feature importance is defined as the decrease in a model score when a single feature value is adjusted (permuted). This is achieved by permuting input features one-at-a-time, predict our model outputs with the permuted input feature, comparing the original model outputs with the permuted model outputs.\n",
    "\n",
    "Here we will try four different types of permutation. Three types of permutation aim to eliminate the signal of a specific input feature by taking the minimum, mean and maximum of the input feature, whereas the final permutation type aims to introduce a lot of noise to the signal of a specific input feature by shuffling the dates around. Here we assess the difference between the original model outputs and the permuted model outputs using the mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.style import use\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "mean_importance = {\"feature\": [],\n",
    "                   \"importance\": [],}\n",
    "\n",
    "for input_feature in X_test.columns:\n",
    "    # Mean premutation importance:\n",
    "    # Takes the input feature mean to determine performance\n",
    "    X_test_permuted = X_test.copy()\n",
    "    X_test_permuted[input_feature] = X_test_permuted[input_feature].mean()\n",
    "    \n",
    "    permutation = {\"date\": X_test.index,\n",
    "                  \"actual\": X_test[input_feature],\n",
    "                  \"permuted\": X_test_permuted[input_feature]}\n",
    "    permutation = pd.DataFrame(permutation)\n",
    "    \n",
    "    # Plot the permutation\n",
    "    use('ggplot')\n",
    "    fig, ax = plt.subplots()\n",
    "    permutation.plot.line(x=\"date\",\n",
    "                         ax=ax)\n",
    "    ax.set_title(\"Actual vs Permuted\")\n",
    "    ax.set_ylabel(f\"{input_feature}\")\n",
    "    ax.set_xlabel(\"Date\")\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    # Register the feature importance\n",
    "    y_pred_permuted = model.predict(X_test_permuted)\n",
    "    mse_permuted = metrics.mean_squared_error(y_pred, y_pred_permuted)\n",
    "    \n",
    "    mean_importance[\"feature\"].append(input_feature)\n",
    "    mean_importance[\"importance\"].append(mse_permuted)\n",
    "    print(f'Permutation (mean) importance of {input_feature}: {mse_permuted}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 Engaging some brain cells\n",
    "Now that I have shown how to calculate the mean permutation feature importance, do the same for the minimum, maximum and shuffle feature importance in the three code blocks below. Just copy the above code and adjust where necessary. Make sure you register the importance information to the correct dictionary: *minimum_importance*, *maximum_importance* and *shuffle_importance* respectively. A quick tip is to take a look at the [DataFrame sample function](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sample.html) for the shuffle importance. If today is not the day to engage your braincells, you can see the answer in the text below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum_importance = {\"feature\": [],\n",
    "                   \"importance\": [],}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum_importance = {\"feature\": [],\n",
    "                   \"importance\": [],}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_importance = {\"feature\": [],\n",
    "                   \"importance\": [],}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click to see the minimum solution</summary>\n",
    "    ``` python\n",
    "    import sklearn.metrics as metrics\n",
    "    import matplotlib.pyplot as plt\n",
    "    from matplotlib.style import use\n",
    "    import matplotlib.dates as mdates\n",
    "\n",
    "    minimum_importance = {\"feature\": [],\n",
    "                    \"importance\": [],}\n",
    "\n",
    "    for input_feature in X_test.columns:\n",
    "        X_test_permuted = X_test.copy()\n",
    "        X_test_permuted[input_feature] = X_test_permuted[input_feature].min()\n",
    "        \n",
    "        permutation = {\"date\": X_test.index,\n",
    "                    \"actual\": X_test[input_feature],\n",
    "                    \"permuted\": X_test_permuted[input_feature]}\n",
    "        permutation = pd.DataFrame(permutation)\n",
    "        \n",
    "        # Plot the permutation\n",
    "        use('ggplot')\n",
    "        fig, ax = plt.subplots()\n",
    "        permutation.plot.line(x=\"date\",\n",
    "                            ax=ax)\n",
    "        ax.set_title(\"Actual vs Permuted\")\n",
    "        ax.set_ylabel(f\"{input_feature}\")\n",
    "        ax.set_xlabel(\"Date\")\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "        fig.tight_layout()\n",
    "        \n",
    "        # Register the feature importance\n",
    "        y_pred_permuted = model.predict(X_test_permuted)\n",
    "        mse_permuted = metrics.mean_squared_error(y_pred, y_pred_permuted)\n",
    "        \n",
    "        minimum_importance[\"feature\"].append(input_feature)\n",
    "        minimum_importance[\"importance\"].append(mse_permuted)\n",
    "        print(f'Permutation (minimum) importance of {input_feature}: {mse_permuted}')\n",
    "    ```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click to see the maximum solution</summary>\n",
    "    ``` python\n",
    "    import sklearn.metrics as metrics\n",
    "    import matplotlib.pyplot as plt\n",
    "    from matplotlib.style import use\n",
    "    import matplotlib.dates as mdates\n",
    "\n",
    "    maximum_importance = {\"feature\": [],\n",
    "                    \"importance\": [],}\n",
    "\n",
    "    for input_feature in X_test.columns:\n",
    "        X_test_permuted = X_test.copy()\n",
    "        X_test_permuted[input_feature] = X_test_permuted[input_feature].max()\n",
    "        \n",
    "        permutation = {\"date\": X_test.index,\n",
    "                    \"actual\": X_test[input_feature],\n",
    "                    \"permuted\": X_test_permuted[input_feature]}\n",
    "        permutation = pd.DataFrame(permutation)\n",
    "        \n",
    "        # Plot the permutation\n",
    "        use('ggplot')\n",
    "        fig, ax = plt.subplots()\n",
    "        permutation.plot.line(x=\"date\",\n",
    "                            ax=ax)\n",
    "        ax.set_title(\"Actual vs Permuted\")\n",
    "        ax.set_ylabel(f\"{input_feature}\")\n",
    "        ax.set_xlabel(\"Date\")\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "        fig.tight_layout()\n",
    "        \n",
    "        # Register the feature importance\n",
    "        y_pred_permuted = model.predict(X_test_permuted)\n",
    "        mse_permuted = metrics.mean_squared_error(y_pred, y_pred_permuted)\n",
    "        \n",
    "        maximum_importance[\"feature\"].append(input_feature)\n",
    "        maximum_importance[\"importance\"].append(mse_permuted)\n",
    "        print(f'Permutation (maximum) importance of {input_feature}: {mse_permuted}')\n",
    "    ```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click to see the shuffle solution</summary>\n",
    "    ``` python\n",
    "    import sklearn.metrics as metrics\n",
    "    import matplotlib.pyplot as plt\n",
    "    from matplotlib.style import use\n",
    "    import matplotlib.dates as mdates\n",
    "\n",
    "    shuffle_importance = {\"feature\": [],\n",
    "                    \"importance\": [],}\n",
    "\n",
    "    for input_feature in X_test.columns:\n",
    "        X_test_permuted = X_test.copy()\n",
    "        X_test_permuted[input_feature] = X_test_permuted[input_feature].sample(frac = 1).values\n",
    "        \n",
    "        permutation = {\"date\": X_test.index,\n",
    "                    \"actual\": X_test[input_feature],\n",
    "                    \"permuted\": X_test_permuted[input_feature]}\n",
    "        permutation = pd.DataFrame(permutation)\n",
    "        \n",
    "        # Plot the permutation\n",
    "        use('ggplot')\n",
    "        fig, ax = plt.subplots()\n",
    "        permutation.plot.line(x=\"date\",\n",
    "                            ax=ax)\n",
    "        ax.set_title(\"Actual vs Permuted\")\n",
    "        ax.set_ylabel(f\"{input_feature}\")\n",
    "        ax.set_xlabel(\"Date\")\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "        fig.tight_layout()\n",
    "        \n",
    "        # Register the feature importance\n",
    "        y_pred_permuted = model.predict(X_test_permuted)\n",
    "        mse_permuted = metrics.mean_squared_error(y_pred, y_pred_permuted)\n",
    "        \n",
    "        shuffle_importance[\"feature\"].append(input_feature)\n",
    "        shuffle_importance[\"importance\"].append(mse_permuted)\n",
    "        print(f'Permutation (shuffle) importance of {input_feature}: {mse_permuted}')\n",
    "    ```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Lets finally plot the importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.style import use\n",
    "\n",
    "mean_importance = pd.DataFrame(mean_importance)\n",
    "minimum_importance = pd.DataFrame(minimum_importance)\n",
    "maximum_importance = pd.DataFrame(maximum_importance)\n",
    "shuffle_importance = pd.DataFrame(shuffle_importance)\n",
    "\n",
    "permutation_importance = mean_importance[['feature']]\n",
    "permutation_importance[\"mean\"] = mean_importance['importance']\n",
    "permutation_importance[\"minimum\"] = minimum_importance['importance']\n",
    "permutation_importance[\"maximum\"] = maximum_importance['importance']\n",
    "permutation_importance[\"shuffle\"] = shuffle_importance['importance']\n",
    "\n",
    "# Plot the feature importance\n",
    "use('ggplot')\n",
    "fig, ax = plt.subplots()\n",
    "permutation_importance.plot.bar(x=\"feature\",\n",
    "                                ax=ax)\n",
    "ax.set_title(\"Permutation feature importance\")\n",
    "ax.set_ylabel(\"Mean squared error\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 SHAP importance\n",
    "Lastly we take a look at SHAP importance values. The SHAP (SHapley Additive exPlanations) package is a popular Python library used for interpreting the output of machine learning models. It provides a unified framework for explaining the predictions made by black-box models. SHAP values, in particular, quantify the contribution of each feature to the prediction made by the model. They provide a measure of feature importance and help in understanding the impact of individual features on the model's output.\n",
    "\n",
    "Here we first need to install the SHAP package (especially on Google Colab) if it is not yet installed. Then we build a SHAP explainer and use our test dataset to generate the feature importance values. *Note this may take some time!* Afterwards, we plot the feature importance of our model using the SHAP build-in features plotting functions [beeswarm](https://shap.readthedocs.io/en/latest/example_notebooks/api_examples/plots/beeswarm.html) and [bar](https://shap.readthedocs.io/en/latest/example_notebooks/api_examples/plots/bar.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "explainer = shap.Explainer(model)\n",
    "shap_values = explainer(X_test)\n",
    "shap_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.beeswarm(shap_values)\n",
    "shap.plots.bar(shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1 Engage some braincells\n",
    "SHAP also provides functionality to plot the *attribution* (both positive and negative) of the input features *for ever single prediction our model has made* using the [waterfall](https://shap.readthedocs.io/en/latest/example_notebooks/api_examples/plots/waterfall.html). Look at the documentation and figure out how to make a waterfall plot for the 105th date in the code block below. If today is not the day to engage your braincells, you can see the answer in the text below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click to see the solution</summary>\n",
    "    ```python\n",
    "    date_index = 105\n",
    "    shap.plots.waterfall(shap_values[date_index])\n",
    "    ```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "codegeo_workshops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
